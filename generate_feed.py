#!/usr/bin/env python3
"""
GÃ¼nlÃ¼k RSS Feed Ãœretici
- Google Ads & Marketing
- Yapay Zeka (AI)
- SEO
- Google Haberleri
GÃ¼nde 15 haber, filtrelenmiÅŸ ve profesyonel.
"""

import feedparser
import datetime
import hashlib
import re
import html
from xml.etree.ElementTree import Element, SubElement, tostring, indent

# â”€â”€â”€ RSS KaynaklarÄ± â”€â”€â”€
FEEDS = {
    "Yapay Zeka": [
        "https://news.google.com/rss/search?q=yapay+zeka+OR+artificial+intelligence&hl=tr&gl=TR&ceid=TR:tr",
        "https://news.google.com/rss/search?q=AI+technology+news&hl=en&gl=US&ceid=US:en",
    ],
    "Google Ads & Marketing": [
        "https://news.google.com/rss/search?q=google+ads+marketing&hl=tr&gl=TR&ceid=TR:tr",
        "https://news.google.com/rss/search?q=digital+marketing+google+ads&hl=en&gl=US&ceid=US:en",
    ],
    "SEO": [
        "https://news.google.com/rss/search?q=SEO+arama+motoru+optimizasyonu&hl=tr&gl=TR&ceid=TR:tr",
        "https://news.google.com/rss/search?q=SEO+search+engine+optimization+news&hl=en&gl=US&ceid=US:en",
    ],
    "Google Haberleri": [
        "https://news.google.com/rss/search?q=Google+update+news&hl=tr&gl=TR&ceid=TR:tr",
        "https://news.google.com/rss/search?q=Google+company+news+update&hl=en&gl=US&ceid=US:en",
    ],
}

# â”€â”€â”€ Kategori aÃ§Ä±klamalarÄ± (zengin iÃ§erik iÃ§in) â”€â”€â”€
CATEGORY_CONTEXT = {
    "Yapay Zeka": "Bu geliÅŸme, yapay zeka teknolojilerinin hÄ±zla ilerlemesiyle birlikte sektÃ¶rde Ã¶nemli bir deÄŸiÅŸim sinyali veriyor. AI alanÄ±ndaki yenilikler, iÅŸ dÃ¼nyasÄ±ndan saÄŸlÄ±ÄŸa kadar pek Ã§ok sektÃ¶rÃ¼ doÄŸrudan etkiliyor.",
    "Google Ads & Marketing": "Dijital reklamcÄ±lÄ±k dÃ¼nyasÄ±nda sÃ¼rekli geliÅŸen stratejiler ve araÃ§lar, markalarÄ±n hedef kitlelerine daha etkili ulaÅŸmasÄ±nÄ± saÄŸlÄ±yor. Google Ads ekosistemindeki gÃ¼ncellemeler, pazarlamacÄ±lar iÃ§in yeni fÄ±rsatlar sunuyor.",
    "SEO": "Arama motoru optimizasyonu sÃ¼rekli deÄŸiÅŸen algoritma gÃ¼ncellemeleriyle ÅŸekilleniyor. Web sitelerinin organik gÃ¶rÃ¼nÃ¼rlÃ¼ÄŸÃ¼ iÃ§in en gÃ¼ncel SEO stratejilerini takip etmek kritik Ã¶nem taÅŸÄ±yor.",
    "Google Haberleri": "Google'Ä±n Ã¼rÃ¼n ve hizmetlerindeki gÃ¼ncellemeler, milyarlarca kullanÄ±cÄ±yÄ± ve iÅŸletmeyi doÄŸrudan etkiliyor. Teknoloji dÃ¼nyasÄ±nÄ±n en bÃ¼yÃ¼k oyuncusundan gelen her yenilik, dijital ekosistemin geleceÄŸini ÅŸekillendiriyor.",
}

# â”€â”€â”€ Filtreleme: Ä°stenmeyen kelimeler â”€â”€â”€
BLOCKED_KEYWORDS = [
    "kumar", "bahis", "casino", "sex", "porno", "dedikodu",
    "magazin", "astroloji", "burÃ§", "falcÄ±", "ÅŸok eden",
    "inanÄ±lmaz", "aldatan", "skandal", "Ã§Ä±plak", "yasak",
    "gambling", "porn", "nsfw", "scandal", "shocking",
    "clickbait", "lottery", "horoscope",
]

# â”€â”€â”€ GÃ¼venilir kaynak Ã¶nceliÄŸi â”€â”€â”€
TRUSTED_SOURCES = [
    "techcrunch", "theverge", "arstechnica", "wired", "reuters",
    "bloomberg", "searchengineland", "searchenginejournal", "moz.com",
    "semrush", "ahrefs", "hubspot", "nytimes", "bbc", "cnn",
    "webrazzi", "shiftdelete", "donanimhaber", "chip.com.tr",
    "marketingturkiye", "digitalage", "google.com/blog",
    "blog.google", "developers.google", "support.google",
]


def is_blocked(title: str, summary: str = "") -> bool:
    text = (title + " " + summary).lower()
    return any(kw in text for kw in BLOCKED_KEYWORDS)


def trust_score(link: str) -> int:
    link_lower = link.lower()
    for i, src in enumerate(TRUSTED_SOURCES):
        if src in link_lower:
            return len(TRUSTED_SOURCES) - i
    return 0


def clean_html(raw: str) -> str:
    clean = re.sub(r"<[^>]+>", "", raw)
    return html.unescape(clean).strip()


def extract_source(link: str) -> str:
    """URL'den kaynak adÄ±nÄ± Ã§Ä±kar."""
    try:
        from urllib.parse import urlparse
        domain = urlparse(link).netloc
        domain = domain.replace("www.", "")
        parts = domain.split(".")
        if len(parts) >= 2:
            return parts[-2].capitalize()
        return domain.capitalize()
    except:
        return ""


def build_rich_description(title: str, summary: str, category: str, link: str) -> str:
    """Zengin ve bilgi verici aÃ§Ä±klama oluÅŸtur."""
    source = extract_source(link)
    context = CATEGORY_CONTEXT.get(category, "")
    
    # Ã–zet varsa kullan, yoksa kategori baÄŸlamÄ±nÄ± ekle
    if summary and len(summary) > 50:
        desc = f"ğŸ“Œ {category} | Kaynak: {source}\n\n{summary}"
    else:
        desc = f"ğŸ“Œ {category} | Kaynak: {source}\n\n{title}. {context}"
    
    # AÃ§Ä±klamayÄ± zenginleÅŸtir
    if len(desc) < 200:
        desc += f"\n\nğŸ’¡ {context}"
    
    return desc[:600]


def fetch_all_entries() -> list:
    all_entries = []
    seen_titles = set()

    for category, urls in FEEDS.items():
        for url in urls:
            try:
                feed = feedparser.parse(url)
                for entry in feed.entries:
                    title = clean_html(entry.get("title", ""))
                    summary = clean_html(entry.get("summary", ""))
                    link = entry.get("link", "")

                    if len(title) < 10:
                        continue

                    if is_blocked(title, summary):
                        continue

                    title_hash = hashlib.md5(
                        title.lower()[:50].encode()
                    ).hexdigest()
                    if title_hash in seen_titles:
                        continue
                    seen_titles.add(title_hash)

                    pub_date = entry.get("published_parsed")
                    if pub_date:
                        pub_dt = datetime.datetime(*pub_date[:6])
                    else:
                        pub_dt = datetime.datetime.now()

                    # Zengin aÃ§Ä±klama oluÅŸtur
                    rich_desc = build_rich_description(title, summary, category, link)

                    all_entries.append(
                        {
                            "title": title,
                            "summary": rich_desc,
                            "link": link,
                            "category": category,
                            "pub_dt": pub_dt,
                            "trust": trust_score(link),
                            "source": extract_source(link),
                        }
                    )
            except Exception as e:
                print(f"Hata ({category}): {e}")

    return all_entries


def select_top_entries(entries: list, total: int = 15) -> list:
    categories = list(FEEDS.keys())
    per_category = total // len(categories)
    remainder = total % len(categories)

    selected = []

    for i, cat in enumerate(categories):
        cat_entries = [e for e in entries if e["category"] == cat]
        cat_entries.sort(key=lambda x: (-x["trust"], -x["pub_dt"].timestamp()))
        count = per_category + (1 if i < remainder else 0)
        selected.extend(cat_entries[:count])

    selected.sort(key=lambda x: -x["pub_dt"].timestamp())
    return selected[:total]


def generate_rss_xml(entries: list) -> str:
    rss = Element("rss", version="2.0")
    rss.set("xmlns:atom", "http://www.w3.org/2005/Atom")

    channel = SubElement(rss, "channel")
    SubElement(channel, "title").text = "Dijital DÃ¼nya - GÃ¼nlÃ¼k Haberler"
    SubElement(channel, "description").text = (
        "Yapay Zeka, Google Ads, SEO ve Google haberleri - GÃ¼nlÃ¼k 15 seÃ§me haber"
    )
    SubElement(channel, "language").text = "tr"
    SubElement(channel, "generator").text = "Custom RSS Aggregator"
    SubElement(channel, "ttl").text = "1440"

    CATEGORY_EMOJI = {
        "Yapay Zeka": "ğŸ¤–",
        "Google Ads & Marketing": "ğŸ“¢",
        "SEO": "ğŸ”",
        "Google Haberleri": "ğŸ”µ",
    }

    for entry in entries:
        item = SubElement(channel, "item")
        emoji = CATEGORY_EMOJI.get(entry["category"], "ğŸ“°")
        SubElement(item, "title").text = f'{emoji} {entry["title"]}'
        SubElement(item, "link").text = entry["link"]
        SubElement(item, "description").text = entry["summary"]
        SubElement(item, "category").text = entry["category"]
        # pubDate kasÄ±tlÄ± olarak eklenmedi - saat bilgisi gÃ¶sterilmeyecek
        SubElement(item, "guid", isPermaLink="false").text = hashlib.md5(
            entry["link"].encode()
        ).hexdigest()

    indent(rss, space="  ")
    xml_str = '<?xml version="1.0" encoding="UTF-8"?>\n' + tostring(
        rss, encoding="unicode"
    )
    return xml_str


def main():
    print("ğŸ“¡ RSS haberleri Ã§ekiliyor...")
    entries = fetch_all_entries()
    print(f"  â†’ Toplam {len(entries)} haber bulundu")

    print("ğŸ” Filtreleniyor ve seÃ§iliyor...")
    top_entries = select_top_entries(entries, total=15)
    print(f"  â†’ {len(top_entries)} haber seÃ§ildi")

    for e in top_entries:
        print(f'  [{e["category"]}] {e["title"][:60]}...')

    print("ğŸ“ RSS XML oluÅŸturuluyor...")
    xml_content = generate_rss_xml(top_entries)

    with open("docs/feed.xml", "w", encoding="utf-8") as f:
        f.write(xml_content)

    print("âœ… docs/feed.xml baÅŸarÄ±yla oluÅŸturuldu!")


if __name__ == "__main__":
    main()
